{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# IMPORTS\n",
    "import gzip\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "from random import seed\n",
    "from operator import itemgetter\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXACT ALGORITHM\n",
    "'''\n",
    "* This is the exact algorithm for calculating frequent items from a dataset.\n",
    "* The algorithm follows exactly from class\n",
    "* The output returns a list of tuples in the form (item, freq), the runtime\n",
    "  of the algorithm, and then the dataset size \n",
    "  Note: runtime does not include time to return/output any values\n",
    "* The runtime and dataset size will be used later in the notebook.\n",
    "'''\n",
    "def exact_freq_items(thres, data):\n",
    "\n",
    "    # get start time of algorithm to calculate runtime later\n",
    "    start_time = time.time()\n",
    "\n",
    "    # create dict to store support of each item\n",
    "    support = dict()\n",
    "    # create a counter to measure dsize as we read thru data\n",
    "    dsize = 0\n",
    "\n",
    "    # open file and read transactions line by line\n",
    "    # increment dsize by 1 for each transaction\n",
    "    # loop thru transaction and increment support for each item present\n",
    "    with open(data, 'rt') as reader:\n",
    "        for transaction in reader:\n",
    "            \n",
    "            #increment dataset size counter\n",
    "            dsize += 1\n",
    "\n",
    "            # split string into the items it contains\n",
    "            # split() strips whitespace and newlines\n",
    "            elements = transaction.split()\n",
    "\n",
    "            # increment support for each item in transaction\n",
    "            # if item not in dict, add it then increment\n",
    "            # else just increment\n",
    "            for element in elements:\n",
    "                if element in support.keys():\n",
    "                    support[element] += 1\n",
    "                else:\n",
    "                    support[element] = 1\n",
    "    \n",
    "    # for each item in support, calculate frequency\n",
    "    # calculate by dividing support of item by dsize\n",
    "    # if frequency is greater than thres, add tuple to output\n",
    "    # tuple consists of item value followed by frequency\n",
    "    output = []\n",
    "    for element in support:\n",
    "        if support[element] / dsize >= thres:\n",
    "            output.append((int(element), support[element] / dsize))\n",
    "\n",
    "    # sort output by value of item in ascending order\n",
    "    # there can't be any ties\n",
    "    output = sorted(output, key=itemgetter(0))\n",
    "\n",
    "    # Sort output by frequency in descending order, ties broken by value of item \n",
    "    # python sorted() is \"stable\" so initial order is preserved during ties,\n",
    "    #   and original order is ascending by value of item\n",
    "    output = sorted(output, key= itemgetter(1), reverse=True) \n",
    "    \n",
    "    # get runtime\n",
    "    # multiply by 1000 to return value in ms and round to 3 decimals\n",
    "    final_time = round(1000 * (time.time() - start_time), 3)\n",
    "\n",
    "    # print size of dataset & runtime\n",
    "    #print(\"Size of dataset: \" + str(dsize))\n",
    "    #print(\"Runtime --- %s ms ---\" % final_time)\n",
    "\n",
    "    # return tuple of items and their frequencies\n",
    "    return output, final_time, dsize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run exact algorithm on webdocs dataset, ascribe id = 1\n",
    "# store output, runtime, and dataset size\n",
    "\n",
    "output1, runtime1, dsize1 = exact_freq_items(0, \"./data/webdocs.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run exact algorithm on kosarak dataset, ascribe id = 2\n",
    "# store output, runtime, and dataset size\n",
    "\n",
    "output2, runtime2, dsize2 = exact_freq_items(0, \"./data/kosarak.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run exact algorithm on accidents dataset, ascribe id = 3\n",
    "# store output, runtime, and dataset size\n",
    "\n",
    "output3, runtime3, dsize3 = exact_freq_items(0, \"./data/accidents.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot results\n",
    "'''\n",
    "* We will plot the farction of the alphabet which is frequenct vs the threshold frequency thres\n",
    "* To achieve this we ran the exact frequent items algorithm on the three datasets with the \n",
    "  threshold equal to zero so we got the exact frequency of every item\n",
    "* Each frequency will end up being an x coordinate of a point on the graph where the number \n",
    "  of items with a frequency greater or equal to that frequency divided by the dataset size \n",
    "  is the y coordinate.\n",
    "* Since the output is sorted in descending order, we will plot the most frequent item with the\n",
    "  point (freq1, 1/dsize). The next point will be (freq2, 2/dsize). All until (freq_smallest, dsize/dsize).\n",
    "'''\n",
    "\n",
    "# the following lists will store x and y axis variables for the 3 datasets\n",
    "x1 = []\n",
    "y1 = []\n",
    "\n",
    "x2 = []\n",
    "y2 = []\n",
    "\n",
    "x3 = []\n",
    "y3 = []\n",
    "\n",
    "# store size of the alphabet\n",
    "# this is the len of the output\n",
    "size1 = len(output1)\n",
    "# this counter helps determine farction to plot on y axis\n",
    "counter = 1\n",
    "# loop thru the output, each output is a point \n",
    "for freq in output1:\n",
    "    # append the frequency as the x coordinate for the point\n",
    "    x1.append(freq[1])\n",
    "    # append the number of items with a greater frequency over size as y coordinate\n",
    "    y1.append(counter/size1)\n",
    "    # increment counter\n",
    "    counter += 1\n",
    "\n",
    "# same method as previous dataset\n",
    "size2 = len(output2)\n",
    "counter = 1\n",
    "for freq in output2:\n",
    "    x2.append(freq[1])\n",
    "    y2.append(counter/size2)\n",
    "    counter += 1\n",
    "\n",
    "# same method as previous dataset\n",
    "size3 = len(output3)\n",
    "counter = 1\n",
    "for freq in output3:\n",
    "    x3.append(freq[1])\n",
    "    y3.append(counter/size3)\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "# plot the three lines with different styles (colors automatic)\n",
    "# label each line for legend\n",
    "plt.plot(x1,y1, label=\"webdocs\", linestyle=\"-\")\n",
    "plt.plot(x2,y2, label=\"kosarak\", linestyle=\"--\")\n",
    "plt.plot(x3,y3, label=\"accidents\", linestyle=\"-.\")\n",
    "# make y axis a log scale\n",
    "plt.yscale(\"log\")\n",
    "# show legend and then plot\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling algorithm\n",
    "'''\n",
    "* This is the sampling approximation algorithm for finding frequent items from a dataset\n",
    "* The algorithm follows exactly from in class\n",
    "* The random sample is taken using the python random.sample() method which returns a\n",
    "  list of random indexes. We then sort them so we can more quickly find the next transaction. \n",
    "* Algorithm reads the data in line by line and if transaction number is the first element of \n",
    "  the random sample array then it processes it, otherwise it skips that line. The first element\n",
    "  of the array is then removed and we check for the new first element of the array\n",
    "* The algorithm runs until the random sample number array is empty, then it calculates epsilon\n",
    "  and return outputs with the threshold thres-eps/2.\n",
    "* The output is the runtime, epsilon, and then the frequent items in the form of (item, freq) \n",
    "'''\n",
    "\n",
    "def sample_freq_items(ssize, dsize, delta, thres, data):\n",
    "    \n",
    "    seed(198)\n",
    "\n",
    "    # get start time of algorithm to calculate runtime later\n",
    "    start_time = time.time()\n",
    "\n",
    "    # get ssize random integers from the range [0,dsize)\n",
    "    samples = random.sample(range(dsize),k=ssize)\n",
    "    # sort the random integers so first element is always next needed transaction\n",
    "    samples.sort()\n",
    "\n",
    "    # create a doct to store support of all the items\n",
    "    support = dict()\n",
    "\n",
    "    # pop the first element of the samples array\n",
    "    # this is the next transaction to be processed\n",
    "    next = samples.pop(0)\n",
    "    # this stores the transaction that is currently being read\n",
    "    current = 0\n",
    "\n",
    "    # this stores the number of elements in the transaction with the most elements\n",
    "    max_t = 0\n",
    "\n",
    "    # start reading in the file line by line (transaction by transaction)\n",
    "    with open(data, 'rt') as reader:\n",
    "        for transaction in reader:\n",
    "\n",
    "            # if the current transaction is the next sample, process it\n",
    "            if current == next:\n",
    "\n",
    "                # split the transaction (a string) into its elements\n",
    "                elements = transaction.split()\n",
    "\n",
    "                # if lenght of the transaction is largest so far, store it\n",
    "                if len(elements) > max_t:\n",
    "                    max_t = len(elements)\n",
    "\n",
    "                # for each element in the transaction, increment its support\n",
    "                for element in elements:\n",
    "                    # if element already in dict, just incremenet\n",
    "                    # otherwise, create a key and store value 1\n",
    "                    if element in support:\n",
    "                        support[element] += 1\n",
    "                    else:\n",
    "                        support[element] = 1\n",
    "                \n",
    "                # if there are no samples remaining, break from the loop\n",
    "                # we're done, no need to read rest of file\n",
    "                if len(samples) == 0:\n",
    "                    break\n",
    "                # if elements still remain,\n",
    "                # pop next element of sample array to find next transaction in sample\n",
    "                next = samples.pop(0)\n",
    "            \n",
    "            # incremenet current to progress to next transaction\n",
    "            current += 1\n",
    "\n",
    "    # reading file and getting support has completed\n",
    "\n",
    "    # calculate ds using the max_t found during reading\n",
    "    ds = math.floor(math.log(max_t,2) + 1)\n",
    "    # calculate epsilon using ds, delta, and ssize\n",
    "    eps = math.sqrt(2*(ds + math.log(1/delta)) / ssize)\n",
    "\n",
    "    # create a lsit to store the output\n",
    "    output = []\n",
    "\n",
    "    # for each item that appeared in the data:\n",
    "    # caluclate frequency by dividing support by sample size\n",
    "    # if frequency is greater than thres - epsilon/2, add to output\n",
    "    # add output in the form (item, freq)\n",
    "    for element in support:\n",
    "        if support[element] / ssize >= thres - eps/2:\n",
    "            output.append((int(element), support[element] / ssize))\n",
    "\n",
    "    # sort output by value of item in ascending order\n",
    "    # there can't be any ties\n",
    "    output = sorted(output, key=itemgetter(0))\n",
    "\n",
    "    # sort output by frequency in descending order, ties broken by value of item \n",
    "    # python sorted() is \"stable\" so order from intitial sort is preserved during ties,\n",
    "    #   and that order is ascending by value of item\n",
    "    output = sorted(output, key= itemgetter(1), reverse=True)\n",
    "\n",
    "    # get runtime\n",
    "    # multiply by 1000 to return value in ms and round to 3 decimals\n",
    "    runtime = round(1000 * (time.time() - start_time), 3)\n",
    "\n",
    "    # print runtime\n",
    "    #print(\"Runtime --- %s ms ---\" % runtime)\n",
    "\n",
    "    return runtime, eps, output\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sampling algorithm\n",
    "'''\n",
    "* Here we will analyze two things: runtime and error vs error bound.\n",
    "* We collect all the data simultaneously to avoid unecessary runs\n",
    "* The sampling algorithm is run 5 times for each sample size (we \n",
    "  default have 4 sample size, but this can be changed simply by\n",
    "  altering the ssizes list to include more ints)\n",
    "* For each run, the output, runtime, and eps is stored\n",
    "* After the 5 runs for each sample size; the max, min, and avg\n",
    "  runtimes are stored; the max epsilon is stored; and the output\n",
    "  is stored in three places (each a separate copy to avoid overwriting)\n",
    "* Next the exact algorithm is run twice with threshold thres and \n",
    "  thres-epsilon\n",
    "* Then we subtract the output from the exact run from each output of \n",
    "  each sampling runs and take the absolute value to get absolute error\n",
    "* The max, min, and avg are then calculated for each run, and then for \n",
    "  each sample size for every frequent item and stored. These will be plotted.\n",
    "* The max, min, and avg runtimes are then plotted on a log-log graph and compared\n",
    "  to the exact algorithm runtime\n",
    "* Then the absolute errors are plotted vs the epsilon/2 theoretical error bound\n",
    "'''\n",
    "\n",
    "# parameters that determine sample size and threshold\n",
    "thres = 0.8\n",
    "ssizes = [100,1000,10000,100000]\n",
    "\n",
    "# lists to store max, min, avg runtimes\n",
    "max_runs = []\n",
    "min_runs = []\n",
    "avg_runs = []\n",
    "# list to store max eps values\n",
    "max_eps_runs = []\n",
    "# lists to store max, min, avg abs errors\n",
    "max_err_runs = []\n",
    "min_err_runs = []\n",
    "avg_err_runs = []\n",
    "\n",
    "# loop thru each sample size\n",
    "for ssize in ssizes:\n",
    "    # create temporary lists to store values from runs on this sample size\n",
    "    runtimes = []\n",
    "    epss = []\n",
    "    max_outputs = []\n",
    "    min_outputs = []\n",
    "    avg_outputs = []\n",
    "\n",
    "    # run the sampling algorithm 5 times for each sample size\n",
    "    for run in range(0,5):\n",
    "\n",
    "        # run the sampling algorithm\n",
    "        runtime,eps,output = sample_freq_items(ssize, dsize3, 0.1, thres,\"./data/accidents.txt\")\n",
    "\n",
    "        # add runtime and eps value to corresponding lists\n",
    "        runtimes.append(runtime)\n",
    "        epss.append(eps)\n",
    "\n",
    "        # get frequencies from the outputs\n",
    "        outputs = [item[1] for item in output]\n",
    "        # store a copy of the frequencies in each of max, min, avg outputs\n",
    "        # needs copies otherwise future calculation are done on same list\n",
    "        max_outputs.append(outputs.copy())\n",
    "        min_outputs.append(outputs.copy())\n",
    "        avg_outputs.append(outputs.copy())\n",
    "\n",
    "    # calculate and store max, min, and avg runtimes for this sample size\n",
    "    max_runs.append(max(runtimes))\n",
    "    min_runs.append(min(runtimes))\n",
    "    avg_runs.append(mean(runtimes))\n",
    "    # calculate and store max epsilon value for this sample size\n",
    "    max_eps_runs.append(max(epss))\n",
    "\n",
    "    # add outputs for this sample size to abs error lists\n",
    "    max_err_runs.append(max_outputs)\n",
    "    min_err_runs.append(min_outputs)\n",
    "    avg_err_runs.append(avg_outputs)\n",
    "    \n",
    "# run the exact algorithm twice with thres and thres - epsilon\n",
    "# use max epsilon from smallest samnple size for second run\n",
    "output, runtime_eps, dsize = exact_freq_items(thres, \"./data/accidents.txt\")\n",
    "output_eps2, runtime_eps2, dsize_eps2 = exact_freq_items(thres-max_eps_runs[0], \"./data/accidents.txt\")\n",
    "\n",
    "# loop thru all ouputs to calculate abs errors then find max, min, avg\n",
    "# loop thru sample sizes\n",
    "for i in range(0,len(ssizes)):\n",
    "    # loop thru each of 5 runs\n",
    "    for j in range(0,5):\n",
    "        # loop thru each frequent item outputted\n",
    "        for k in range(0, len(max_err_runs[i][j])):\n",
    "            # subtract exact algorithm ouput from sample output and take abs value\n",
    "            max_err_runs[i][j][k] = abs(max_err_runs[i][j][k] - output_eps2[k][1])\n",
    "            min_err_runs[i][j][k] = abs(min_err_runs[i][j][k] - output_eps2[k][1])\n",
    "            avg_err_runs[i][j][k] =abs(avg_err_runs[i][j][k] - output_eps2[k][1])\n",
    "\n",
    "        # calulcate max, min, avg abs error of each individual run\n",
    "        # then store it in place of the run output list\n",
    "        max_err_runs[i][j] = max(max_err_runs[i][j])\n",
    "        min_err_runs[i][j] = min(min_err_runs[i][j])\n",
    "        avg_err_runs[i][j] = mean(avg_err_runs[i][j])\n",
    "\n",
    "    # calculate the max, min, avs abs error for each sample size\n",
    "    # then store it in place of sample size output list\n",
    "    max_err_runs[i] = max(max_err_runs[i])\n",
    "    min_err_runs[i] = min(min_err_runs[i])\n",
    "    avg_err_runs[i] = mean(avg_err_runs[i])\n",
    "        \n",
    "# Plot the lines for the runtimes of the sampling algorithm\n",
    "plt.plot(ssizes, max_runs, label=\"max runtime\", linestyle=\":\")\n",
    "plt.plot(ssizes, min_runs, label=\"min runtime\", linestyle=\"--\")\n",
    "plt.plot(ssizes, avg_runs, label=\"avg runtime\", linestyle=\"-.\")\n",
    "# plot a horizontal line of exact runtime\n",
    "plt.plot([ssizes[0],ssizes[-1]], [runtime_eps, runtime_eps], label=\"exact\", linestyle=\"-\")\n",
    "# give plot a title and axis labels\n",
    "plt.title(\"Runtime vs Sample Size\")\n",
    "plt.xlabel(\"Sample size\")\n",
    "plt.ylabel(\"Runtime (ms)\")\n",
    "# give both axes a log scale\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "# show legend and plot\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot the max, min, and avg error for each sample size\n",
    "plt.plot(ssizes, max_err_runs, label=\"max error\", linestyle=\":\")\n",
    "plt.plot(ssizes, min_err_runs, label=\"min error\", linestyle=\"--\")\n",
    "plt.plot(ssizes, avg_err_runs, label=\"avg error\", linestyle=\"-.\")\n",
    "# divide max eps by 2 for all sample sizes and plot them as a line\n",
    "max_eps_runs2 = [x/2 for x in max_eps_runs]\n",
    "plt.plot(ssizes, max_eps_runs, label=\"max eps/2\", linestyle=\"-\")\n",
    "# give plot a title and axis labels\n",
    "plt.title(\"Error vs epsilon/2\")\n",
    "plt.xlabel(\"Sample size\")\n",
    "plt.ylabel(\"Error\")\n",
    "# give both axes a log scale\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "# show legend and plot\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}